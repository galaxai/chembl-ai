services:
  spark-master:
    image: apache/spark:4.1.0
    container_name: spark-master
    ports:
      - '8080:8080'
      - '7077:7077'
      - '15002:15002'

    command: |
      bash -c '
        set -e
        export HOME=/tmp
        export PYTHONUSERBASE=/tmp/.local
        python3 -m ensurepip --upgrade || true
        python3 -m pip install --user --no-cache-dir "rdkit>=2025.9.3" "numpy>=2.2.0" "pyarrow>=22.0.0"

        # Start the Spark Master daemon
        /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master &

        # Start Spark Connect server for remote client connections
        /opt/spark/sbin/start-connect-server.sh \
          --master spark://spark-master:7077 \
          --conf spark.executor.memory=2g \
          --conf spark.connect.grpc.binding.address=0.0.0.0 \
          --conf spark.connect.grpc.binding.port=15002 &

        # Keep container running
        wait'

    hostname: spark-master
    volumes:
        - ../data/chembl_36/:/data/chembl_36/
    networks:
      - spark-net

  mlflow:
    image: ghcr.io/mlflow/mlflow:v3.8.1
    container_name: mlflow
    ports:
      - "5000:5000"
    command: >
      mlflow server --host 0.0.0.0 --port 5000
      --backend-store-uri sqlite:////mlruns/mlflow.db
      --default-artifact-root /mlruns
      --allowed-hosts mlflow,mlflow:5000,localhost,localhost:5000,127.0.0.1
    volumes:
      - ./mlruns:/mlruns
    networks:
      - spark-net

  airflow:
    image: apache/airflow:3.1.6-python3.12
    container_name: airflow-standalone
    env_file:
      - .env
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__AUTH_MANAGER=airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager
      - _AIRFLOW_DB_MIGRATE=true
      - _AIRFLOW_WWW_USER_CREATE=true
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - MLFLOW_ARTIFACT_URI=/mlruns
      - PIP_ADDITIONAL_REQUIREMENTS=apache-airflow-providers-fab==3.1.1
    ports:
      - "8081:8080"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./mlruns:/mlruns
    command: >
      bash -c 'airflow db migrate &&
      pip install --no-cache-dir mlflow==3.8.1 &&
      ((airflow users create --username "${_AIRFLOW_WWW_USER_USERNAME}" --password "${_AIRFLOW_WWW_USER_PASSWORD}" --firstname "${_AIRFLOW_WWW_USER_FIRSTNAME}" --lastname "${_AIRFLOW_WWW_USER_LASTNAME}" --role Admin --email "${_AIRFLOW_WWW_USER_EMAIL}" 2>/dev/null) || true) &&
      airflow api-server -p 8080 &
      airflow scheduler &
      airflow dag-processor &
      wait'
    depends_on:
      - spark-master
      - mlflow
    networks:
      - spark-net

  spark-worker-1:
    image: apache/spark:4.1.0
    container_name: spark-worker-1
    environment:
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=1
    depends_on:
      - spark-master
    command: |
      bash -c '
        set -e
        export HOME=/tmp
        export PYTHONUSERBASE=/tmp/.local
        python3 -m ensurepip --upgrade || true
        python3 -m pip install --user --no-cache-dir "rdkit>=2025.9.3" "numpy>=2.2.0" "pyarrow>=22.0.0"
        /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077'
    hostname: spark-worker-1
    volumes:
      - ../data/chembl_36/:/data/chembl_36/
    networks:
      - spark-net

  spark-worker-2:
    image: apache/spark:4.1.0
    container_name: spark-worker-2
    environment:
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=1
    depends_on:
      - spark-master
    command: |
      bash -c '
        set -e
        export HOME=/tmp
        export PYTHONUSERBASE=/tmp/.local
        python3 -m ensurepip --upgrade || true
        python3 -m pip install --user --no-cache-dir "rdkit>=2025.9.3" "numpy>=2.2.0" "pyarrow>=22.0.0"
        /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077'
    hostname: spark-worker-2
    volumes:
      - ../data/chembl_36/:/data/chembl_36/
    networks:
      - spark-net

  spark-worker-3:
    image: apache/spark:4.1.0
    container_name: spark-worker-3
    environment:
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=1
    depends_on:
      - spark-master
    command: |
      bash -c '
        set -e
        export HOME=/tmp
        export PYTHONUSERBASE=/tmp/.local
        python3 -m ensurepip --upgrade || true
        python3 -m pip install --user --no-cache-dir "rdkit>=2025.9.3" "numpy>=2.2.0" "pyarrow>=22.0.0"
        /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077'
    hostname: spark-worker-3
    volumes:
      - ../data/chembl_36/:/data/chembl_36/
    networks:
      - spark-net


  spark-worker-4:
    image: apache/spark:4.1.0
    container_name: spark-worker-4
    environment:
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=1
    depends_on:
      - spark-master
    command: |
      bash -c '
        set -e
        export HOME=/tmp
        export PYTHONUSERBASE=/tmp/.local
        python3 -m ensurepip --upgrade || true
        python3 -m pip install --user --no-cache-dir "rdkit>=2025.9.3" "numpy>=2.2.0" "pyarrow>=22.0.0"
        /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077'
    hostname: spark-worker-4
    volumes:
      - ../data/chembl_36/:/data/chembl_36/
    networks:
      - spark-net


  spark-worker-5:
    image: apache/spark:4.1.0
    container_name: spark-worker-5
    environment:
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=1
    depends_on:
      - spark-master
    command: |
      bash -c '
        set -e
        export HOME=/tmp
        export PYTHONUSERBASE=/tmp/.local
        python3 -m ensurepip --upgrade || true
        python3 -m pip install --user --no-cache-dir "rdkit>=2025.9.3" "numpy>=2.2.0" "pyarrow>=22.0.0"
        /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077'
    hostname: spark-worker-5
    volumes:
      - ../data/chembl_36/:/data/chembl_36/
    networks:
      - spark-net

  spark-worker-6:
    image: apache/spark:4.1.0
    container_name: spark-worker-6
    environment:
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=1
    depends_on:
      - spark-master
    command: |
      bash -c '
        set -e
        export HOME=/tmp
        export PYTHONUSERBASE=/tmp/.local
        python3 -m ensurepip --upgrade || true
        python3 -m pip install --user --no-cache-dir "rdkit>=2025.9.3" "numpy>=2.2.0" "pyarrow>=22.0.0"
        /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077'
    hostname: spark-worker-6
    volumes:
      - ../data/chembl_36/:/data/chembl_36/
    networks:
      - spark-net

  spark-worker-7:
    image: apache/spark:4.1.0
    container_name: spark-worker-7
    environment:
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=1
    depends_on:
      - spark-master
    command: |
      bash -c '
        set -e
        export HOME=/tmp
        export PYTHONUSERBASE=/tmp/.local
        python3 -m ensurepip --upgrade || true
        python3 -m pip install --user --no-cache-dir "rdkit>=2025.9.3" "numpy>=2.2.0" "pyarrow>=22.0.0"
        /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077'
    hostname: spark-worker-7
    volumes:
      - ../data/chembl_36/:/data/chembl_36/
    networks:
      - spark-net

  spark-worker-8:
    image: apache/spark:4.1.0
    container_name: spark-worker-8
    environment:
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=1
    depends_on:
      - spark-master
    command: |
      bash -c '
        set -e
        export HOME=/tmp
        export PYTHONUSERBASE=/tmp/.local
        python3 -m ensurepip --upgrade || true
        python3 -m pip install --user --no-cache-dir "rdkit>=2025.9.3" "numpy>=2.2.0" "pyarrow>=22.0.0"
        /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077'
    hostname: spark-worker-8
    volumes:
      - ../data/chembl_36/:/data/chembl_36/
    networks:
      - spark-net


networks:
  spark-net:
    driver: bridge
